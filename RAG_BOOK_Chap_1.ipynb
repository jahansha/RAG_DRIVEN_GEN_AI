{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN28qB15bm9S3kGGNIIdzEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahansha/RAG_DRIVEN_GEN_AI/blob/main/RAG_BOOK_Chap_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3YAxj51j1566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2f3e72-9866-4586-acab-8bace6a4f3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.69.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup API Key storage"
      ],
      "metadata": {
        "id": "juedKpEK1-Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeKVf0ah2e_m",
        "outputId": "c70c1e17-6780-4d6d-996b-7d476304e186"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY = f.readline().strip()\n",
        "f.close()\n",
        "#API_KEY"
      ],
      "metadata": {
        "id": "nXaLY9LU27ie"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The openai key"
      ],
      "metadata": {
        "id": "StPV3Kv96NA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY']=API_KEY\n",
        "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "nbV1uSSr6Sd7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global total_response"
      ],
      "metadata": {
        "id": "yXue-zISZ95H"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    text_input = itext  # JOHNS\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain, read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      #total_response =str(response.model_dump_json(indent=2))  # this has a nice display\n",
        "      print(response.model_dump_json(indent=2))  # this has a nice display\n",
        "      #print(response)   # unstructured display\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "7ytKHR4YBH8p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "QcaMVtd9BOaV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATION WITHOUT AUGMENTATION"
      ],
      "metadata": {
        "id": "3kx820DSBriZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store:\"\n",
        "query += \"A RAG vector store is a database that contains vectorized data points.\"\n",
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hULW8JhBVnO",
        "outputId": "7534a7a0-d619-4b98-e7dd-f94b55b464b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-BHXN0GWvMnWSMnQxI8Sdtc3utqdit\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"Certainly! Let's break down the concept of a RAG vector store and what it means in the context of data storage and retrieval.\\n\\n### RAG Vector Store\\n\\n**RAG** stands for **Retrieval-Augmented Generation**, which is a technique used in natural language processing (NLP) and machine learning to enhance the generation of text by incorporating external information retrieved from a database or knowledge base. The RAG model combines retrieval mechanisms with generative models to produce more accurate and contextually relevant outputs.\\n\\n#### Vector Store\\n\\nA **vector store** is a type of database specifically designed to store and manage vectorized data points. In the context of machine learning and NLP, vectorized data refers to data that has been transformed into a numerical format, typically as vectors, which can be easily processed by algorithms. This transformation is crucial for tasks such as similarity search, clustering, and classification.\\n\\n### Key Components of a RAG Vector Store\\n\\n1. **Vectorization**: This is the process of converting data (such as text, images, or other forms of information) into vectors. In NLP, this often involves using techniques like word embeddings (e.g., Word2Vec, GloVe) or more advanced models like BERT or GPT to represent words, sentences, or documents as vectors in a high-dimensional space.\\n\\n2. **Storage**: The vector store holds these vectorized representations. It is optimized for efficient storage and retrieval of high-dimensional vectors. This often involves using specialized data structures and indexing techniques to facilitate fast similarity searches.\\n\\n3. **Retrieval**: The retrieval component is responsible for finding and returning the most relevant vectors from the store based on a given query. This is typically done using similarity measures such as cosine similarity or Euclidean distance to compare the query vector with vectors in the store.\\n\\n4. **Augmentation**: In the context of RAG, the retrieved information is used to augment the input to a generative model. This means that the generative model can use the additional context or knowledge retrieved from the vector store to produce more informed and accurate outputs.\\n\\n### Applications\\n\\n- **Question Answering**: RAG models can retrieve relevant documents or snippets from a vector store to provide more accurate answers to user queries.\\n- **Content Generation**: By incorporating external information, RAG models can generate more contextually rich and informative content.\\n- **Recommendation Systems**: Vector stores can be used to find similar items or content based on user preferences or behavior.\\n\\n### Benefits\\n\\n- **Enhanced Contextual Understanding**: By leveraging external information, RAG models can generate outputs that are more aligned with real-world knowledge.\\n- **Scalability**: Vector stores are designed to handle large volumes of data, making them suitable for applications requiring extensive knowledge bases.\\n- **Efficiency**: Optimized retrieval mechanisms ensure that relevant information can be accessed quickly, even from large datasets.\\n\\nIn summary, a RAG vector store is a specialized database that plays a crucial role in enhancing the capabilities of generative models by providing them with access to a rich repository of vectorized information, enabling more accurate and contextually aware outputs.\",\n",
            "        \"refusal\": null,\n",
            "        \"role\": \"assistant\",\n",
            "        \"annotations\": [],\n",
            "        \"audio\": null,\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1743519550,\n",
            "  \"model\": \"gpt-4o-2024-08-06\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": \"default\",\n",
            "  \"system_fingerprint\": \"fp_6dd05565ef\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 631,\n",
            "    \"prompt_tokens\": 63,\n",
            "    \"total_tokens\": 694,\n",
            "    \"completion_tokens_details\": {\n",
            "      \"accepted_prediction_tokens\": 0,\n",
            "      \"audio_tokens\": 0,\n",
            "      \"reasoning_tokens\": 0,\n",
            "      \"rejected_prediction_tokens\": 0\n",
            "    },\n",
            "    \"prompt_tokens_details\": {\n",
            "      \"audio_tokens\": 0,\n",
            "      \"cached_tokens\": 0\n",
            "    }\n",
            "  }\n",
            "}\n",
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down the concept of a RAG vector store and what it means\n",
            "in the context of data storage and retrieval.  ### RAG Vector Store  **RAG**\n",
            "stands for **Retrieval-Augmented Generation**, which is a technique used in\n",
            "natural language processing (NLP) and machine learning to enhance the generation\n",
            "of text by incorporating external information retrieved from a database or\n",
            "knowledge base. The RAG model combines retrieval mechanisms with generative\n",
            "models to produce more accurate and contextually relevant outputs.  #### Vector\n",
            "Store  A **vector store** is a type of database specifically designed to store\n",
            "and manage vectorized data points. In the context of machine learning and NLP,\n",
            "vectorized data refers to data that has been transformed into a numerical\n",
            "format, typically as vectors, which can be easily processed by algorithms. This\n",
            "transformation is crucial for tasks such as similarity search, clustering, and\n",
            "classification.  ### Key Components of a RAG Vector Store  1. **Vectorization**:\n",
            "This is the process of converting data (such as text, images, or other forms of\n",
            "information) into vectors. In NLP, this often involves using techniques like\n",
            "word embeddings (e.g., Word2Vec, GloVe) or more advanced models like BERT or GPT\n",
            "to represent words, sentences, or documents as vectors in a high-dimensional\n",
            "space.  2. **Storage**: The vector store holds these vectorized representations.\n",
            "It is optimized for efficient storage and retrieval of high-dimensional vectors.\n",
            "This often involves using specialized data structures and indexing techniques to\n",
            "facilitate fast similarity searches.  3. **Retrieval**: The retrieval component\n",
            "is responsible for finding and returning the most relevant vectors from the\n",
            "store based on a given query. This is typically done using similarity measures\n",
            "such as cosine similarity or Euclidean distance to compare the query vector with\n",
            "vectors in the store.  4. **Augmentation**: In the context of RAG, the retrieved\n",
            "information is used to augment the input to a generative model. This means that\n",
            "the generative model can use the additional context or knowledge retrieved from\n",
            "the vector store to produce more informed and accurate outputs.  ###\n",
            "Applications  - **Question Answering**: RAG models can retrieve relevant\n",
            "documents or snippets from a vector store to provide more accurate answers to\n",
            "user queries. - **Content Generation**: By incorporating external information,\n",
            "RAG models can generate more contextually rich and informative content. -\n",
            "**Recommendation Systems**: Vector stores can be used to find similar items or\n",
            "content based on user preferences or behavior.  ### Benefits  - **Enhanced\n",
            "Contextual Understanding**: By leveraging external information, RAG models can\n",
            "generate outputs that are more aligned with real-world knowledge. -\n",
            "**Scalability**: Vector stores are designed to handle large volumes of data,\n",
            "making them suitable for applications requiring extensive knowledge bases. -\n",
            "**Efficiency**: Optimized retrieval mechanisms ensure that relevant information\n",
            "can be accessed quickly, even from large datasets.  In summary, a RAG vector\n",
            "store is a specialized database that plays a crucial role in enhancing the\n",
            "capabilities of generative models by providing them with access to a rich\n",
            "repository of vectorized information, enabling more accurate and contextually\n",
            "aware outputs.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## this section defines some data and does cosine affinity tests.\n",
        "It does not do RAG."
      ],
      "metadata": {
        "id": "xnCnDVCgVN3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "3J87gpEbe842"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "NmnHZjr8fKTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advnced techniques and Evaluation"
      ],
      "metadata": {
        "id": "NOPFLmFnlPc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        use_idf=True,\n",
        "        norm='l2',\n",
        "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
        "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
        "    )\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]"
      ],
      "metadata": {
        "id": "ei0f5_JWlWKK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced Similarity"
      ],
      "metadata": {
        "id": "8n5mCMjkloH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmatized_words = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        lemmatized_words.append(token.lemma_)\n",
        "    return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "    expanded_words = words.copy()\n",
        "    for word in words:\n",
        "        expanded_words.extend(get_synonyms(word))\n",
        "    return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvFGf3vgls7U",
        "outputId": "11585443-73ea-4211-e78f-f36ada65c16c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}